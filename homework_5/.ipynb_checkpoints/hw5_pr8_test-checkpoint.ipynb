{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross entropy error E_out over 100 runs:  0.1143084578180361\n",
      "average number of epochs:  355.0\n",
      "Average alternative E_out over 100 runs:  0.101\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def theta(s):\n",
    "    return 1 / (1 + np.exp(-s))\n",
    "\n",
    "def problem8_9():\n",
    "    \n",
    "    RUNS = 1\n",
    "    E_out_total = 0\n",
    "    epoch_total = 0\n",
    "    E_out_alternative_total = 0\n",
    "    \n",
    "    for run in range(RUNS):\n",
    "        # create training set with N = 100 points via separating line\n",
    "\n",
    "        # separating line,\n",
    "        # choose two random points A, B in [-1,1] x [-1,1]\n",
    "        A = np.random.uniform(-1,1,2)\n",
    "        B = np.random.uniform(-1,1,2)\n",
    "\n",
    "        # the line can be described by y = m*x + b where m is the slope\n",
    "        m = (B[1] - A[1]) / (B[0] - A[0])\n",
    "        b = B[1] - m * B[0]  \n",
    "        w_f = np.array([b, m, -1])\n",
    "\n",
    "        #-----------------------\n",
    "\n",
    "        # Pick N data points (x, y) uniformly from the box [-1,1] x [-1,1]\n",
    "        N = 100\n",
    "        x1 = np.random.uniform(-1,1,N)\n",
    "        x2 = np.random.uniform(-1,1,N)\n",
    "\n",
    "        X = np.transpose(np.array([np.ones(N), x1, x2]))           # input\n",
    "\n",
    "        # Classify these points\n",
    "        y_f = np.sign(np.dot(X, w_f))\n",
    "\n",
    "        #-----------------------\n",
    "\n",
    "        # Run logistic regression\n",
    "        # initialize weights for hypothesis with zeros\n",
    "        eta = 0.01\n",
    "        w_g = np.zeros(3)       # weight vector for hypothesis g\n",
    "\n",
    "        # start iterations\n",
    "        for t in range(10**5):\n",
    "            \n",
    "            # create permutation of data points\n",
    "            indices = list(range(N))\n",
    "            random.shuffle(indices)\n",
    "            w_old = w_g\n",
    "\n",
    "            # for each epoch\n",
    "            for index in indices:\n",
    "                xn = X[index, :]                 # pick a point\n",
    "                yn = y_f[index]\n",
    "                delta_w = -yn * xn / (1 + math.exp(yn * np.dot(w_g.T, xn)))\n",
    "\n",
    "                # update w\n",
    "                w_g = w_g - eta * delta_w\n",
    "\n",
    "            # after epoch check how much w_g changed\n",
    "            # print(\"t = \", t, \"    diff_w = \", np.linalg.norm(w_g - w_old))\n",
    "            if np.linalg.norm(w_g - w_old) < 0.01:\n",
    "                break\n",
    "\n",
    "        epoch_total += t\n",
    "\n",
    "\n",
    "        \n",
    "        # Generate 1000 test points to calculate E_out\n",
    "        N_test = 1000\n",
    "        x1_test = np.random.uniform(-1,1,N_test)                    # 1000 points\n",
    "        x2_test = np.random.uniform(-1,1,N_test)\n",
    "        X_test = np.array([np.ones(N_test), x1_test, x2_test]).T    # feature matrix\n",
    "        \n",
    "        y_f_test = np.sign(np.dot(X_test, w_f))                     # true classification\n",
    "        \n",
    "        # Calculate E_out via cross entropy error\n",
    "        E_out = 0\n",
    "        for i in range(N_test):\n",
    "            E_out += math.log(1 + math.exp(-y_f_test[i] * np.dot(X_test[i,:], w_g)))\n",
    "        \n",
    "        E_out_total += (E_out / N_test)\n",
    "    \n",
    "        \n",
    "        #xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "        # Calculate E_out differently by simulating the noise\n",
    "        # for each test point x we predict the probability that y = 1\n",
    "        # via the theta function: theta(s) = 1 / (1 + e^(-s)),\n",
    "        # see slide 16 of lecture 9\n",
    "        # the probability is then theta(w'x)\n",
    "        y_prob = theta(np.dot(X_test, w_g))\n",
    "        \n",
    "        # simulate a classification by our hypothesis g\n",
    "        y_g_class = []\n",
    "        for prob in y_prob:\n",
    "            if np.random.uniform(0,1) < prob:\n",
    "                y_g_class.append(1)\n",
    "            else:\n",
    "                y_g_class.append(-1)\n",
    "                \n",
    "        # Compare the classification by g to true classifications made by target function f\n",
    "        E_out_alternative_total += sum(y_g_class != y_f_test) / N_test \n",
    "        #xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "    \n",
    "    \n",
    "    E_out_avg = E_out_total / RUNS\n",
    "    E_out_alternative_avg = E_out_alternative_total / RUNS\n",
    "    epoch_avg = epoch_total / RUNS\n",
    "    \n",
    "    return (E_out_avg, epoch_avg, E_out_alternative_avg)\n",
    "\n",
    "\n",
    "E_out_avg, epoch_avg, E_out_alternative_avg = problem8_9()\n",
    "print(\"Average cross entropy error E_out over 100 runs: \", E_out_avg)\n",
    "print(\"average number of epochs: \", epoch_avg)\n",
    "print(\"Average alternative E_out over 100 runs: \", E_out_alternative_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
